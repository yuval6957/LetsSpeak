{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Core modules for LetsSpeak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import os\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "from gtts import gTTS\n",
    "import openai\n",
    "import tiktoken\n",
    "import whisper\n",
    "\n",
    "from reinautils import Parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if os.path.isfile(\"/home/notebooks/LetsSpeak/tokens.json\"):\n",
    "    params = Parameters().from_json(\"/home/notebooks/LetsSpeak/tokens.json\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = params.tokens.openai.research\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SpeechToText:\n",
    "    \"\"\"\n",
    "    _summary_: Speech to text using huggingface's wav2vec2 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = whisper.load_model(\"base\")\n",
    "    \n",
    "    def __call__(self, file):\n",
    "        return self.model.transcribe(file)['text']\n",
    "    \n",
    "class TextToSpeech:\n",
    "    \"\"\"\n",
    "    _summary_: Text to speech using Google's TTS.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lang='en'):\n",
    "        self.model = gTTS\n",
    "        self.lang = lang\n",
    "    \n",
    "    def __call__(self, text, file):\n",
    "        tts = self.model(text=text, lang=self.lang, slow=False)\n",
    "        tts.save(file)\n",
    "        return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stt = SpeechToText()\n",
    "tts = TextToSpeech()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What is the meaning of life?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(stt(\"/home/hd/LetsSpeak/What-is-the-meaning-of-life.mp3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hd/LetsSpeak/the_meaning_of_life.mp3'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts(text=\"The meaning of life is 42\", file = \"/home/hd/LetsSpeak/the_meaning_of_life.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\", return_num_tokens=False):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   output = openai.Embedding.create(input = [text], model=model)\n",
    "   return (output['data'][0]['embedding'], output[\"usage\"][\"total_tokens\"]) if return_num_tokens else output['data'][0]['embedding']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ContextPool:\n",
    "    \"\"\"\n",
    "    A class used to store and retrieve text and their corresponding embeddings.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    contexts : list\n",
    "        A list storing the text data\n",
    "    embeddings : np.array\n",
    "        A numpy array storing the embeddings of the text\n",
    "    file_name : str\n",
    "        The file name to load data from and save data to\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    add(text: str)\n",
    "        Adds the text to the contexts list and its corresponding embedding to the embeddings list.\n",
    "    get(index: int)\n",
    "        Returns the text and its corresponding embedding at the given index.\n",
    "    __len__()\n",
    "        Returns the length of the contexts list.\n",
    "    pop(index: int)\n",
    "        Removes the text and its corresponding embedding at the given index.\n",
    "    __call__(text: str, n: int)\n",
    "        Returns the 'n' most similar contexts to the given text based on their embeddings.\n",
    "    save()\n",
    "        Saves the contexts and embeddings to a file.\n",
    "    load()\n",
    "        Loads the contexts and embeddings from a file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_name=None, model=\"text-embedding-ada-002\"):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the ContextPool object.\n",
    "\n",
    "        Parameters:\n",
    "            file_name (str): The file name to load data from and save data to.\n",
    "        \"\"\"\n",
    "        self.contexts = []\n",
    "        self.embeddings = np.array([])\n",
    "        self.num_tokens = np.array([[]])\n",
    "        self.file_name = file_name\n",
    "        self.model = model\n",
    "        self.get_embedding = partial(get_embedding, model=model)\n",
    "        if self.file_name:\n",
    "            self.load()\n",
    "\n",
    "    def add(self, text):\n",
    "        \"\"\"\n",
    "        Adds the text to the contexts list and its corresponding embedding to the embeddings list.\n",
    "        \"\"\"\n",
    "        self.contexts.append(text)\n",
    "        embedding, num_tokens = self.get_embedding(text, return_num_tokens=True)\n",
    "        if self.embeddings.shape[0]:\n",
    "            self.embeddings = np.append(self.embeddings, np.array(embedding)[None,:], axis=0)\n",
    "        else:\n",
    "            self.embeddings = np.array(embedding)[None,:]\n",
    "        self.num_tokens = np.append(self.num_tokens, num_tokens)\n",
    "\n",
    "    def get(self, index):\n",
    "        \"\"\"\n",
    "        Returns the text and its corresponding embedding at the given index.\n",
    "\n",
    "        Parameters:\n",
    "            index (int): The index of the desired text and embedding.\n",
    "\n",
    "        Returns:\n",
    "            (str, np.array): The text and its corresponding embedding at the given index.\n",
    "        \"\"\"\n",
    "        return self.contexts[index], self.embeddings[index], self.num_tokens[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the contexts list.\n",
    "        \"\"\"\n",
    "        return len(self.contexts)\n",
    "\n",
    "    def pop(self, index):\n",
    "        \"\"\"\n",
    "        Removes the text and its corresponding embedding at the given index.\n",
    "        \"\"\"\n",
    "        self.contexts.pop(index)\n",
    "        self.embeddings = np.delete(self.embeddings, index)\n",
    "        self.num_tokens = np.delete(self.num_tokens, index)\n",
    "\n",
    "    def __call__(self, text, n=1, threshold=0, too_close_threshold=0.9):\n",
    "        \"\"\"\n",
    "        Returns the 'n' most similar contexts to the given text based on their embeddings.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The text to find the most similar contexts to.\n",
    "            n (int): The number of most similar contexts to return.\n",
    "            threshold (float): The minimum similarity required to return a context (between 0 and 1)\n",
    "\n",
    "        Returns:\n",
    "            (index or list of indexs): The 'n' most similar contexts to the given text.\n",
    "        \"\"\"\n",
    "        if len(self.embeddings) == 0:\n",
    "            return []\n",
    "        similarity = np.dot(self.embeddings, get_embedding(text))\n",
    "        \n",
    "        if n==1:\n",
    "            m = np.argmax(similarity)\n",
    "            return [m] if similarity[m] > threshold else []\n",
    "        else:\n",
    "            m = np.argsort(similarity)[-2*n:]\n",
    "            return [i for i in m if (similarity[i] > threshold) and (similarity[i] < too_close_threshold)][:n]\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        Saves the contexts and embeddings to a file.\n",
    "        \"\"\"\n",
    "        if self.file_name:\n",
    "            with open(self.file_name, 'wb') as f:\n",
    "                pickle.dump((self.contexts, self.embeddings, self.num_tokens), f)\n",
    "        else:\n",
    "            print(\"Error: No file name specified for saving.\")\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Loads the contexts and embeddings from a file.\n",
    "        \"\"\"\n",
    "        if self.file_name:\n",
    "            try:\n",
    "                with open(self.file_name, 'rb') as f:\n",
    "                    self.contexts, self.embeddings, self.num_tokens = pickle.load(f)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: File '{self.file_name}' not found. Starting with empty contexts and embeddings.\")\n",
    "        else:\n",
    "            print(\"Error: No file name specified for loading. Is this your first time using this context?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CreateContext:\n",
    "    \"\"\"\n",
    "    This class is responsible for assembling the full context text\n",
    "    from the context pool and the request text.\n",
    "\n",
    "    Attributes:\n",
    "    context_pool (ContextPool): A ContextPool object that stores contexts and their embeddings.\n",
    "    general_prompt (str): General prompt text that starts the assembled text.\n",
    "    context_prompt (str): Context prompt text that introduces the contexts.\n",
    "    request_prompt (str): Request prompt text that introduces the request.\n",
    "    max_contexts_tokens (int): Maximum number of tokens that contexts can have in total.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, context_pool, general_prompt=None, context_prompt=None, request_prompt=None, last_n=2, max_contexts_tokens=512, threshold=0.5, too_close_threshold=0.95):\n",
    "        \"\"\"\n",
    "        Initialize CreateContext with a context_pool and optional prompts.\n",
    "\n",
    "        general_prompt, context_prompt, and request_prompt default to predefined texts if not specified.\n",
    "        \"\"\"\n",
    "        self.context_pool = context_pool \n",
    "        self.general_prompt = \"\\nAnswer the request below using the following context\" if general_prompt is None else general_prompt\n",
    "        self.context_prompt = \"\\nContext:\" if context_prompt is None else context_prompt\n",
    "        self.request_prompt = \"\\nRequest:\" if request_prompt is None else request_prompt\n",
    "        self.max_contexts_tokens = max_contexts_tokens\n",
    "        self.threshold = threshold\n",
    "        self.too_close_threshold = too_close_threshold\n",
    "        self.last_n = last_n\n",
    "        \n",
    "    def assemble(self, context, request):\n",
    "        \"\"\"\n",
    "        Assemble the full context text from the given contexts and the request.\n",
    "\n",
    "        Contexts that fit into max_contexts_tokens are joined with newlines,\n",
    "        and the full context is formed with the prompts and the request.\n",
    "        \"\"\"\n",
    "        full_context = []\n",
    "        length = 0\n",
    "        for m in context:\n",
    "            if length + self.context_pool.num_tokens[m] < self.max_contexts_tokens:\n",
    "                full_context.append(self.context_pool.contexts[m])\n",
    "                length += self.context_pool.num_tokens[m]\n",
    "        context_string = \"\\n\".join(full_context)\n",
    "        return f\"{self.general_prompt}\\n{self.context_prompt}\\n{context_string}\\n{self.request_prompt}\\n{request}\\n\"     \n",
    "    \n",
    "    def related_contexts(self, text, n=5):\n",
    "        \"\"\"\n",
    "        Get the contexts that are related to the given text from the context pool.\n",
    "\n",
    "        The number of contexts and similarity thresholds can be adjusted.\n",
    "        \"\"\"\n",
    "        last_context=list(range(len(self.context_pool)))[-self.last_n:]\n",
    "        new_context = self.context_pool(text, n, self.threshold, self.too_close_threshold)\n",
    "        last_context.extend([i for i in new_context if i not in last_context])\n",
    "        return last_context[:n]\n",
    "    \n",
    "    def __call__(self, text, request, n=5):\n",
    "        \"\"\"\n",
    "        Given the text and the request, assemble the full context with related contexts.\n",
    "\n",
    "        This can be conveniently called as an instance object like a function.\n",
    "        \"\"\"\n",
    "        context = self.related_contexts(text, n)\n",
    "        return self.assemble(context, request)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_response(request, create_context, model= \"gpt-3.5-turbo\", n= 5, debug= False):\n",
    "    \"\"\"\n",
    "    Get the response to the given request using the given context pool and model.\n",
    "    \"\"\"\n",
    "    context = create_context(request, request)\n",
    "    if debug:\n",
    "        print(f\"the content is:\\n{context}\\n#####\\n\")\n",
    "    return openai.ChatCompletion.create(model=model, \n",
    "                                        messages =[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": context}])[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def chat(create_context, context_pool, model= \"gpt-3.5-turbo\", n= 5, debug= False, auto_save= True):\n",
    "    \"\"\"\n",
    "    Chat with the given context pool and model.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            request = input(\"Request: \")\n",
    "        except EOFError as e:\n",
    "            break\n",
    "        if request == \"quit\":\n",
    "            break\n",
    "        response = get_response(request, create_context, model, n , debug)\n",
    "        context_pool.add(f\"user:{request}\\nassistant:{response}\")\n",
    "        print(f\"Request: {request}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        if auto_save:\n",
    "            context_pool.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_pool = ContextPool(\"/home/hd/LetsSpeak/context_pool.pkl\")\n",
    "len(context_pool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_context = CreateContext(context_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7537108  0.74636528 0.67878342 0.79604557 0.74621879 0.73172237]\n",
      "Response: Congratulations on launching your podcast, \"Nadav is Talking\"! Since this is your first show, you can modify the greeting to suit the occasion. Here are a few alternative phrases you can use instead of \"welcome back\":\n",
      "\n",
      "1. \"Welcome to the debut episode of 'Nadav is Talking'!\"\n",
      "2. \"Hello and welcome to the premiere of 'Nadav is Talking'!\"\n",
      "3. \"Thank you for tuning in to the very first episode of 'Nadav is Talking'!\"\n",
      "4. \"Welcome, everyone, to the exciting beginning of 'Nadav is Talking'!\"\n",
      "5. \"It's great to have you join us for the inaugural episode of 'Nadav is Talking'!\"\n",
      "\n",
      "Feel free to choose the phrase that resonates with you the most and suits the tone you want to set for your podcast. Best of luck with your first show, and may it be the start of a successful podcasting journey!\n"
     ]
    }
   ],
   "source": [
    "chat(create_context, context_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_pool.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
